# è¯„ä¼°ä¼˜åŒ–å·¥ä½œæµç¨‹ç¤ºä¾‹ - ä½¿ç”¨LangGraphæ„å»ºè‡ªæˆ‘ä¼˜åŒ–çš„å†…å®¹ç”Ÿæˆç³»ç»Ÿ
# åŠŸèƒ½ï¼šç”Ÿæˆç¬‘è¯ â†’ è¯„ä¼°è´¨é‡ â†’ æ ¹æ®åé¦ˆæ”¹è¿› â†’ å¾ªç¯ä¼˜åŒ–ç›´åˆ°æ»¡æ„

import os
import getpass
from typing_extensions import Literal, TypedDict
from langgraph.graph import StateGraph, START, END
from IPython.display import Image, display
from dotenv import load_dotenv
from langchain_qwq import ChatQwQ
from pydantic import BaseModel, Field

# åŠ è½½ç¯å¢ƒå˜é‡é…ç½®
load_dotenv()

# åˆå§‹åŒ–é€šä¹‰åƒé—®LLMæ¨¡å‹
llm = ChatQwQ(
    model="qwen3-4b",                                                    # ä½¿ç”¨qwen3-4bæ¨¡å‹
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",      # é˜¿é‡Œäº‘APIåœ°å€
    max_tokens=3_000,                                                   # æœ€å¤§ç”Ÿæˆtokenæ•°
    timeout=None,                                                       # è¶…æ—¶è®¾ç½®
    max_retries=2,                                                      # æœ€å¤§é‡è¯•æ¬¡æ•°
    # other params...
)

# æ£€æŸ¥å¹¶è®¾ç½®APIå¯†é’¥
if not os.getenv("DASHSCOPE_API_KEY"):
    os.environ["DASHSCOPE_API_KEY"] = getpass.getpass("Enter your Dashscope API key: ")

# çŠ¶æ€å®šä¹‰ - åœ¨å·¥ä½œæµä¸­ä¼ é€’çš„æ•°æ®ç»“æ„
class State(TypedDict):
    joke: str           # ç”Ÿæˆçš„ç¬‘è¯å†…å®¹
    topic: str          # ç¬‘è¯ä¸»é¢˜
    feedback: str       # è¯„ä¼°åé¦ˆä¿¡æ¯
    funny_or_not: str   # è¯„ä¼°ç»“æœï¼ˆfunny/not funnyï¼‰

# è¯„ä¼°åé¦ˆçš„ç»“æ„åŒ–è¾“å‡ºæ¨¡å¼å®šä¹‰
class Feedback(BaseModel):
    grade: Literal["funny", "not funny"] = Field(
        description="åˆ¤æ–­ç¬‘è¯æ˜¯å¦æœ‰è¶£"
    )
    feedback: str = Field(
        description="å¦‚æœç¬‘è¯ä¸å¤Ÿæœ‰è¶£ï¼Œæä¾›æ”¹è¿›å»ºè®®"
    )

# å¢å¼ºLLMä»¥æ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼Œç”¨äºç¬‘è¯è¯„ä¼°
evaluator = llm.with_structured_output(Feedback)

# èŠ‚ç‚¹å‡½æ•°å®šä¹‰ - æ¯ä¸ªèŠ‚ç‚¹è´Ÿè´£ç‰¹å®šçš„å¤„ç†ä»»åŠ¡

def llm_call_generator(state: State):
    """ç¬‘è¯ç”Ÿæˆå™¨èŠ‚ç‚¹ - ç”Ÿæˆæˆ–æ”¹è¿›ç¬‘è¯"""
    if state.get("feedback"):
        # å¦‚æœæœ‰åé¦ˆï¼Œæ ¹æ®åé¦ˆæ”¹è¿›ç¬‘è¯
        msg = llm.invoke(
            f"å†™ä¸€ä¸ªå…³äº{state['topic']}çš„ç¬‘è¯ï¼Œä½†è¦è€ƒè™‘ä»¥ä¸‹åé¦ˆæ„è§ï¼š{state['feedback']}"
        )
    else:
        # é¦–æ¬¡ç”Ÿæˆç¬‘è¯
        msg = llm.invoke(f"å†™ä¸€ä¸ªå…³äº{state['topic']}çš„ç¬‘è¯")

    return {"joke": msg.content}

def llm_call_evaluator(state: State):
    """ç¬‘è¯è¯„ä¼°å™¨èŠ‚ç‚¹ - è¯„ä¼°ç¬‘è¯è´¨é‡å¹¶æä¾›åé¦ˆ"""
    # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºè¯„ä¼°ç¬‘è¯
    grade = evaluator.invoke(f"è¯„ä¼°è¿™ä¸ªç¬‘è¯ï¼š{state['joke']}")

    return {
        "funny_or_not": grade.grade,
        "feedback": grade.feedback
    }

# æ¡ä»¶è¾¹å‡½æ•° - æ ¹æ®è¯„ä¼°ç»“æœå†³å®šä¸‹ä¸€æ­¥æ“ä½œ
def route_joke(state: State):
    """
    è·¯ç”±å†³ç­–å‡½æ•° - æ ¹æ®è¯„ä¼°ç»“æœå†³å®šæ˜¯ç»“æŸè¿˜æ˜¯ç»§ç»­æ”¹è¿›
    è¿”å›å€¼å†³å®šå·¥ä½œæµçš„ä¸‹ä¸€æ­¥èµ°å‘
    """
    if state["funny_or_not"] == "funny":
        return "Accepted"                    # ç¬‘è¯è¢«æ¥å—ï¼Œç»“æŸæµç¨‹
    elif state["funny_or_not"] == "not funny":
        return "Rejected + Feedback"        # ç¬‘è¯è¢«æ‹’ç»ï¼Œè¿”å›ç”Ÿæˆå™¨æ”¹è¿›
    return None


# æ„å»ºä¼˜åŒ–å·¥ä½œæµç¨‹å›¾
optimizer_builder = StateGraph(State)

# æ·»åŠ èŠ‚ç‚¹åˆ°å›¾ä¸­
optimizer_builder.add_node("llm_call_generator", llm_call_generator)    # ç¬‘è¯ç”Ÿæˆå™¨èŠ‚ç‚¹
optimizer_builder.add_node("llm_call_evaluator", llm_call_evaluator)    # ç¬‘è¯è¯„ä¼°å™¨èŠ‚ç‚¹

# æ·»åŠ è¾¹è¿æ¥èŠ‚ç‚¹ - å®šä¹‰æ‰§è¡Œæµç¨‹
optimizer_builder.add_edge(START, "llm_call_generator")                 # ä»STARTå¼€å§‹åˆ°ç”Ÿæˆå™¨
optimizer_builder.add_edge("llm_call_generator", "llm_call_evaluator")  # ç”Ÿæˆå™¨åˆ°è¯„ä¼°å™¨

# æ·»åŠ æ¡ä»¶è¾¹ - æ ¹æ®è¯„ä¼°ç»“æœå†³å®šä¸‹ä¸€æ­¥
optimizer_builder.add_conditional_edges(
    "llm_call_evaluator",    # æ¥æºèŠ‚ç‚¹
    route_joke,              # æ¡ä»¶å‡½æ•°
    {  # è·¯ç”±æ˜ å°„ï¼šroute_jokeè¿”å›çš„åç§° : è¦è®¿é—®çš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹åç§°
        "Accepted": END,                        # æ¥å— â†’ ç»“æŸ
        "Rejected + Feedback": "llm_call_generator",  # æ‹’ç» â†’ é‡æ–°ç”Ÿæˆ
    },
)

# ç¼–è¯‘å·¥ä½œæµ
optimizer_workflow = optimizer_builder.compile()

def simple_llm_chat(user_input: str):
    """
    ç®€å•çš„LLMå¯¹è¯ - ç›´æ¥ä¸ç”¨æˆ·å¯¹è¯
    å‚æ•°: user_input - ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
    """
    response = llm.invoke(user_input)
    print("Assistant:", response.content)

def demo_optimizer_workflow():
    """æ¼”ç¤ºè¯„ä¼°ä¼˜åŒ–å·¥ä½œæµç¨‹ - è‡ªåŠ¨ä¼˜åŒ–ç¬‘è¯è´¨é‡"""
    print("=== è¯„ä¼°ä¼˜åŒ–å·¥ä½œæµç¨‹æ¼”ç¤º ===")
    print("æ­£åœ¨æ˜¾ç¤ºå·¥ä½œæµå›¾å½¢...")

    # æ˜¾ç¤ºå·¥ä½œæµå›¾å½¢
    try:
        display(Image(optimizer_workflow.get_graph().draw_mermaid_png()))
    except Exception as e:
        print(f"æ— æ³•æ˜¾ç¤ºå›¾å½¢: {e}")

    # è·å–ç”¨æˆ·è¾“å…¥çš„ç¬‘è¯ä¸»é¢˜
    topic = input("\nè¯·è¾“å…¥ç¬‘è¯ä¸»é¢˜ (æˆ–ç›´æ¥æŒ‰å›è½¦ä½¿ç”¨é»˜è®¤ä¸»é¢˜'Cats'): ").strip()
    if not topic:
        topic = "Cats"
        print(f"ä½¿ç”¨é»˜è®¤ä¸»é¢˜: {topic}")

    # æ‰§è¡Œä¼˜åŒ–å·¥ä½œæµ
    print(f"\næ­£åœ¨ç”Ÿæˆå…³äº'{topic}'çš„ç¬‘è¯å¹¶è¿›è¡Œè´¨é‡ä¼˜åŒ–...")
    print("æµç¨‹: ç”Ÿæˆç¬‘è¯ â†’ è¯„ä¼°è´¨é‡ â†’ å¦‚éœ€è¦åˆ™æ ¹æ®åé¦ˆæ”¹è¿› â†’ é‡å¤ç›´åˆ°æ»¡æ„")

    state = optimizer_workflow.invoke({"topic": topic})

    print(f"\nä¼˜åŒ–å®Œæˆ!")
    print(f"æœ€ç»ˆè¯„ä¼°ç»“æœ: {state.get('funny_or_not', 'æœªçŸ¥')}")
    print("\n" + "="*50)
    print("æœ€ç»ˆç¬‘è¯:")
    print("="*50)
    print(state["joke"])

    # æ˜¾ç¤ºåé¦ˆä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if state.get("feedback"):
        print(f"\næœ€åä¸€æ¬¡åé¦ˆ: {state['feedback']}")

def interactive_chat():
    """äº¤äº’å¼èŠå¤©åŠŸèƒ½"""
    print("\n=== äº¤äº’å¼èŠå¤©æ¨¡å¼ ===")
    print("è¾“å…¥ 'quit', 'exit' æˆ– 'q' é€€å‡ºèŠå¤©")
    print("-" * 50)

    while True:
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye! å†è§!")
            break
        simple_llm_chat(user_input)
        print()

def interactive_joke_optimizer():
    """äº¤äº’å¼ç¬‘è¯ä¼˜åŒ–å™¨"""
    print("\n=== äº¤äº’å¼ç¬‘è¯ä¼˜åŒ–å™¨ ===")
    print("ç³»ç»Ÿä¼šè‡ªåŠ¨ç”Ÿæˆç¬‘è¯å¹¶è¿›è¡Œè´¨é‡è¯„ä¼°å’Œä¼˜åŒ–")
    print("è¾“å…¥ 'quit', 'exit' æˆ– 'q' é€€å‡º")
    print("-" * 50)

    while True:
        topic = input("è¯·è¾“å…¥ç¬‘è¯ä¸»é¢˜: ").strip()
        if topic.lower() in ["quit", "exit", "q"]:
            print("Goodbye! å†è§!")
            break

        if not topic:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„ç¬‘è¯ä¸»é¢˜")
            continue

        print(f"\næ­£åœ¨ä¸º'{topic}'ç”Ÿæˆå¹¶ä¼˜åŒ–ç¬‘è¯...")
        try:
            state = optimizer_workflow.invoke({"topic": topic})
            print(f"\nä¼˜åŒ–å®Œæˆ! è¯„ä¼°ç»“æœ: {state.get('funny_or_not', 'æœªçŸ¥')}")
            print("\n" + "="*50)
            print("ä¼˜åŒ–åçš„ç¬‘è¯:")
            print("="*50)
            print(state["joke"])

            if state.get("feedback"):
                print(f"\næ”¹è¿›è¿‡ç¨‹ä¸­çš„åé¦ˆ: {state['feedback']}")
            print()
        except Exception as e:
            print(f"ç”Ÿæˆç¬‘è¯æ—¶å‡ºç°é”™è¯¯: {e}")
        print()

def main():
    """ä¸»å‡½æ•° - æä¾›å¤šç§åŠŸèƒ½é€‰æ‹©"""
    print("ğŸš€ LangGraph è¯„ä¼°ä¼˜åŒ–å·¥ä½œæµç¨‹å’Œç¬‘è¯ç”Ÿæˆç³»ç»Ÿ")
    print("=" * 65)

    while True:
        print("\nè¯·é€‰æ‹©åŠŸèƒ½:")
        print("1. æ¼”ç¤ºè¯„ä¼°ä¼˜åŒ–å·¥ä½œæµç¨‹ (å•æ¬¡ç¬‘è¯ä¼˜åŒ–)")
        print("2. äº¤äº’å¼ç¬‘è¯ä¼˜åŒ–å™¨ (è¿ç»­ä¼˜åŒ–ç¬‘è¯)")
        print("3. ç®€å•èŠå¤©æ¨¡å¼")
        print("4. é€€å‡º")

        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()

        if choice == "1":
            demo_optimizer_workflow()
        elif choice == "2":
            interactive_joke_optimizer()
        elif choice == "3":
            interactive_chat()
        elif choice == "4":
            print("æ„Ÿè°¢ä½¿ç”¨! å†è§!")
            break
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-4 ä¹‹é—´çš„æ•°å­—")

if __name__ == "__main__":
    main()
